{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# Install GPU dependencies\n!pip uninstall -y torch torchvision torchaudio torchao trdg pillow transformers datasets opencv-python jiwer\n!pip install torch==2.3.0+cu121 torchvision==0.18.0+cu121 --index-url https://download.pytorch.org/whl/cu121\n!pip install transformers==4.39.3 datasets==3.6.0 opencv-python jiwer pillow>=9.4.0\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import TrOCRProcessor, VisionEncoderDecoderModel\nfrom datasets import load_dataset\nimport cv2\nimport numpy as np\nfrom PIL import Image\nfrom jiwer import cer, wer\nfrom tqdm import tqdm\nimport logging\nimport os\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Setup\nlogging.basicConfig(level=logging.INFO, force=True)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nlogging.info(f\"Using device: {device}\")\nprint(f\"Using device: {device}\")\n\n# Data augmentation\ndef rotate_image(image, angle):\n    (h, w) = image.shape[:2]\n    center = (w // 2, h // 2)\n    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated = cv2.warpAffine(image, M, (w, h))\n    return rotated\n\ndef add_noise(image):\n    noise = np.random.normal(0, 10, image.shape).astype(np.uint8)\n    noisy_image = cv2.add(image, noise)\n    return noisy_image\n\n# Custom dataset\nclass OCRDataset(Dataset):\n    def __init__(self, dataset, processor):\n        self.dataset = dataset\n        self.processor = processor\n        self.valid_indices = self._validate_dataset()\n\n    def _validate_dataset(self):\n        valid_indices = []\n        for idx in range(len(self.dataset)):\n            try:\n                item = self.dataset[idx]\n                image = Image.fromarray(item[\"image\"]).convert(\"RGB\") if not isinstance(item[\"image\"], Image.Image) else item[\"image\"]\n                text = item[\"text\"]\n                if not text or not isinstance(text, str):\n                    continue\n                img_array = np.array(image)\n                if img_array.size == 0:\n                    continue\n                valid_indices.append(idx)\n            except:\n                continue\n        logging.info(f\"Validated dataset: {len(valid_indices)}/{len(self.dataset)} samples valid\")\n        print(f\"Validated dataset: {len(valid_indices)}/{len(self.dataset)} samples valid\")\n        return valid_indices\n\n    def __len__(self):\n        return len(self.valid_indices)\n\n    def __getitem__(self, idx):\n        real_idx = self.valid_indices[idx]\n        item = self.dataset[real_idx]\n        image = Image.fromarray(item[\"image\"]).convert(\"RGB\") if not isinstance(item[\"image\"], Image.Image) else item[\"image\"]\n        text = item[\"text\"]\n\n        img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n        img = cv2.GaussianBlur(img, (3, 3), 0)\n        img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n        img = cv2.convertScaleAbs(img, alpha=1.2, beta=10)\n        img = rotate_image(img, angle=np.random.uniform(-10, 10))\n        img = add_noise(img)\n        img = cv2.resize(img, (384, 384))  # Per report\n        img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n        img = img / 255.0\n        image = Image.fromarray((img * 255).astype(np.uint8))\n\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values.squeeze()\n        labels = self.processor.tokenizer(\n            text, padding=\"max_length\", max_length=128, truncation=True, return_tensors=\"pt\"\n        ).input_ids.squeeze()\n\n        return {\"pixel_values\": pixel_values, \"labels\": labels, \"text\": text}\n\n# Load datasets (full IAM)\ndef load_datasets():\n    iam_dataset = load_dataset(\"alpayariyak/IAM_Sentences\", split=\"train\")\n    iam_dataset = iam_dataset.shuffle(seed=42)\n\n    total_size = len(iam_dataset)\n    train_size = int(0.8 * total_size)  # 80%\n    val_size = int(0.1 * total_size)   # 10%\n    test_size = total_size - train_size - val_size  # 10%\n    train_dataset = OCRDataset(iam_dataset.select(range(train_size)), processor)\n    val_dataset = OCRDataset(iam_dataset.select(range(train_size, train_size + val_size)), processor)\n    test_dataset = OCRDataset(iam_dataset.select(range(train_size + val_size, total_size)), processor)\n\n    logging.info(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    print(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")\n    return train_dataset, val_dataset, test_dataset\n\n# Learning rate scheduler with warmup\ndef get_scheduler(optimizer, num_warmup_steps, total_steps):\n    def lr_lambda(step):\n        if step < num_warmup_steps:\n            return float(step) / float(max(1, num_warmup_steps))\n        return 1.0\n    warmup = LambdaLR(optimizer, lr_lambda)\n    cosine = CosineAnnealingLR(optimizer, T_max=total_steps - num_warmup_steps)\n    return warmup, cosine\n\n# Fine-tune model\ndef fine_tune_model(model, train_loader, val_loader, processor, epochs=20, lr=5e-5):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scaler = GradScaler()\n    model.to(device)\n\n    num_warmup_steps = 3 * len(train_loader)  # 3-epoch warmup\n    total_steps = epochs * len(train_loader)\n    warmup_scheduler, cosine_scheduler = get_scheduler(optimizer, num_warmup_steps, total_steps)\n\n    best_val_loss = float(\"inf\")\n    patience = 10\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n            pixel_values = batch[\"pixel_values\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            with autocast():\n                outputs = model(pixel_values=pixel_values, labels=labels)\n                loss = outputs.loss\n            scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            train_loss += loss.item()\n            if epoch * len(train_loader) + batch_idx < num_warmup_steps:\n                warmup_scheduler.step()\n            else:\n                cosine_scheduler.step()\n            torch.cuda.empty_cache()\n\n        train_loss /= len(train_loader)\n        logging.info(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}\")\n        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f}\")\n\n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for batch in val_loader:\n                pixel_values = batch[\"pixel_values\"].to(device)\n                labels = batch[\"labels\"].to(device)\n                with autocast():\n                    outputs = model(pixel_values=pixel_values, labels=labels)\n                    val_loss += outputs.loss.item()\n        val_loss /= len(val_loader)\n        logging.info(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss:.4f}\")\n        print(f\"Epoch {epoch+1}/{epochs} - Val Loss: {val_loss:.4f}\")\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                logging.info(f\"Stopping early at epoch {epoch+1}, val loss {val_loss:.4f}\")\n                print(f\"Stopping early at epoch {epoch+1}, val loss {val_loss:.4f}\")\n                break\n\n        # Save model every 5 epochs\n        if (epoch + 1) % 5 == 0:\n            try:\n                model.save_pretrained(f\"/kaggle/working/fine_tuned_trocr_epoch_{epoch+1}\")\n                processor.save_pretrained(f\"/kaggle/working/fine_tuned_trocr_epoch_{epoch+1}\")\n                logging.info(f\"Model saved to /kaggle/working/fine_tuned_trocr_epoch_{epoch+1}\")\n                print(f\"Model saved to /kaggle/working/fine_tuned_trocr_epoch_{epoch+1}\")\n            except Exception as e:\n                logging.error(f\"Error saving model at epoch {epoch+1}: {str(e)}\")\n                print(f\"Error saving model at epoch {epoch+1}: {str(e)}\")\n\n# Evaluate model\ndef evaluate_model(model, test_loader, processor):\n    model.eval()\n    predictions = []\n    ground_truths = []\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=\"Evaluating\"):\n            try:\n                pixel_values = batch[\"pixel_values\"].to(device)\n                outputs = model.generate(pixel_values)\n                preds = processor.batch_decode(outputs, skip_special_tokens=True)\n                preds = [p if p else \"<empty>\" for p in preds]\n                predictions.extend(preds)\n                ground_truths.extend(batch[\"text\"])\n            except Exception as e:\n                logging.error(f\"Error in evaluation batch: {str(e)}\")\n                print(f\"Error in evaluation batch: {str(e)}\")\n                continue\n\n    try:\n        cer_score = cer(ground_truths, predictions)\n        wer_score = wer(ground_truths, predictions)\n        logging.info(f\"Test CER: {cer_score:.4f}, WER: {wer_score:.4f}\")\n        print(f\"Test CER: {cer_score:.4f}, WER: {wer_score:.4f}\")\n        return cer_score, wer_score\n    except:\n        logging.error(\"Error computing metrics\")\n        print(\"Error computing metrics\")\n        return None, None\n\n# Main\ndef main():\n    global processor\n    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-handwritten\")\n\n    if model.config.decoder_start_token_id is None:\n        model.config.decoder_start_token_id = processor.tokenizer.cls_token_id or processor.tokenizer.bos_token_id\n    if model.config.pad_token_id is None:\n        model.config.pad_token_id = processor.tokenizer.pad_token_id\n\n    train_dataset, val_dataset, test_dataset = load_datasets()\n\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1)\n    test_loader = DataLoader(test_dataset, batch_size=1)\n\n    fine_tune_model(model, train_loader, val_loader, processor)\n\n    output_dir = \"/kaggle/working/fine_tuned_trocr\"\n    try:\n        model.save_pretrained(output_dir)\n        processor.save_pretrained(output_dir)\n        logging.info(f\"Model and processor saved to {output_dir}\")\n        print(f\"Model and processor saved to {output_dir}\")\n    except Exception as e:\n        logging.error(f\"Error saving model: {str(e)}\")\n        print(f\"Error saving model: {str(e)}\")\n\n    cer_score, wer_score = evaluate_model(model, test_loader, processor)\n    if cer_score is not None and wer_score is not None:\n        logging.info(f\"Final CER: {cer_score:.4f}, WER: {wer_score:.4f}\")\n        print(f\"Final CER: {cer_score:.4f}, WER: {wer_score:.4f}\")\n    else:\n        logging.error(\"Evaluation failed\")\n        print(\"Evaluation failed\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-16T13:05:45.202486Z","iopub.execute_input":"2025-05-16T13:05:45.203118Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.3.0+cu121\nUninstalling torch-2.3.0+cu121:\n  Successfully uninstalled torch-2.3.0+cu121\nFound existing installation: torchvision 0.18.0+cu121\nUninstalling torchvision-0.18.0+cu121:\n  Successfully uninstalled torchvision-0.18.0+cu121\n\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping torchao as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Skipping trdg as it is not installed.\u001b[0m\u001b[33m\n\u001b[0mFound existing installation: pillow 11.0.0\nUninstalling pillow-11.0.0:\n  Successfully uninstalled pillow-11.0.0\nFound existing installation: transformers 4.39.3\nUninstalling transformers-4.39.3:\n  Successfully uninstalled transformers-4.39.3\nFound existing installation: datasets 3.6.0\nUninstalling datasets-3.6.0:\n  Successfully uninstalled datasets-3.6.0\nFound existing installation: opencv-python 4.11.0.86\nUninstalling opencv-python-4.11.0.86:\n  Successfully uninstalled opencv-python-4.11.0.86\nFound existing installation: jiwer 3.1.0\nUninstalling jiwer-3.1.0:\n  Successfully uninstalled jiwer-3.1.0\nLooking in indexes: https://download.pytorch.org/whl/cu121\nCollecting torch==2.3.0+cu121\n  Using cached https://download.pytorch.org/whl/cu121/torch-2.3.0%2Bcu121-cp311-cp311-linux_x86_64.whl (781.0 MB)\nCollecting torchvision==0.18.0+cu121\n  Using cached https://download.pytorch.org/whl/cu121/torchvision-0.18.0%2Bcu121-cp311-cp311-linux_x86_64.whl (7.0 MB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (4.13.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0+cu121) (2.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.0+cu121) (1.26.4)\nCollecting pillow!=8.3.*,>=5.3.0 (from torchvision==0.18.0+cu121)\n  Using cached https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0+cu121) (12.9.41)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0+cu121) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0+cu121) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0+cu121) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0+cu121) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0+cu121) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0+cu121) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0+cu121) (2.4.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0+cu121) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.18.0+cu121) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.18.0+cu121) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.18.0+cu121) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.18.0+cu121) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.18.0+cu121) (2024.2.0)\nUsing cached https://download.pytorch.org/whl/pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\nInstalling collected packages: pillow, torch, torchvision\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchtune 0.6.1 requires datasets, which is not installed.\ndopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, which is not installed.\npeft 0.14.0 requires transformers, which is not installed.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed pillow-11.0.0 torch-2.3.0+cu121 torchvision-0.18.0+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.39.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"INFO:root:Using device: cuda\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\nSome weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-base-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nINFO:root:Validated dataset: 4530/4530 samples valid\n","output_type":"stream"},{"name":"stdout","text":"Validated dataset: 4530/4530 samples valid\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Validated dataset: 566/566 samples valid\n","output_type":"stream"},{"name":"stdout","text":"Validated dataset: 566/566 samples valid\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Validated dataset: 567/567 samples valid\nINFO:root:Dataset sizes: Train=4530, Val=566, Test=567\n","output_type":"stream"},{"name":"stdout","text":"Validated dataset: 567/567 samples valid\nDataset sizes: Train=4530, Val=566, Test=567\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/20: 100%|██████████| 4530/4530 [33:29<00:00,  2.25it/s]\nINFO:root:Epoch 1/20 - Train Loss: 1.2547\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Train Loss: 1.2547\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Epoch 1/20 - Val Loss: 0.8010\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20 - Val Loss: 0.8010\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 4530/4530 [33:34<00:00,  2.25it/s]\nINFO:root:Epoch 2/20 - Train Loss: 0.7747\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Train Loss: 0.7747\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Epoch 2/20 - Val Loss: 0.7539\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/20 - Val Loss: 0.7539\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 4530/4530 [33:34<00:00,  2.25it/s]\nINFO:root:Epoch 3/20 - Train Loss: 0.7634\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Train Loss: 0.7634\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Epoch 3/20 - Val Loss: 0.7751\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/20 - Val Loss: 0.7751\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 4530/4530 [33:38<00:00,  2.24it/s]\nINFO:root:Epoch 4/20 - Train Loss: 0.6560\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Train Loss: 0.6560\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Epoch 4/20 - Val Loss: 0.6268\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/20 - Val Loss: 0.6268\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 4530/4530 [33:11<00:00,  2.28it/s]\nINFO:root:Epoch 5/20 - Train Loss: 0.5176\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Train Loss: 0.5176\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Epoch 5/20 - Val Loss: 0.5932\nRemoved shared tensor {'decoder.output_projection.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/20 - Val Loss: 0.5932\n","output_type":"stream"},{"name":"stderr","text":"INFO:root:Model saved to /kaggle/working/fine_tuned_trocr_epoch_5\n","output_type":"stream"},{"name":"stdout","text":"Model saved to /kaggle/working/fine_tuned_trocr_epoch_5\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20:  11%|█▏        | 513/4530 [03:42<27:45,  2.41it/s]","output_type":"stream"}],"execution_count":null}]}